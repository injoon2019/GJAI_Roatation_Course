{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RM-eg0dEiao"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Flatten, Concatenate, Input, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecXGLAUTHMEQ"
   },
   "source": [
    "# 뉴스 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xt_pnlx5r7W5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# json 읽어오기\n",
    "news_data = pd.read_json('./drive/My Drive/News_Category_Dataset_v2.json', lines=True)\n",
    "# print(news_data)\n",
    "\n",
    "news_data = news_data.loc[:, [\"category\", \"headline\"]]\n",
    "# print(news_data)\n",
    "\n",
    "# 카테고리 정수 인코딩\n",
    "# news_data['category'] = news_data['category'].replace(~~~~~~~, ~~~~~)\n",
    "# print(pd.factorize(news_data['category']))\n",
    "category_list = pd.factorize(news_data['category'])[1]\n",
    "news_data['category'] = pd.factorize(news_data['category'])[0]\n",
    "\n",
    "print(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y060Ub92G8kN"
   },
   "outputs": [],
   "source": [
    "# 정규표현식 사용\n",
    "news_data['headline'] = news_data['headline'].str.replace(\"[^\\w]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2frf1Kn8G9gC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split하면서 shuffle 적용\n",
    "news_train, news_test, y_train, y_test = train_test_split(news_data['headline'], news_data['category'], test_size=0.2, shuffle=True, random_state=23)\n",
    "\n",
    "# 원핫벡터로 만들어줍시다! (num_classes로 카테고리 수 명시 가능)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(len(y_train[0]))\n",
    "print(len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "egb0SZf0G-3h"
   },
   "outputs": [],
   "source": [
    "# 토큰화 진행\n",
    "stopwords = ['a', 'an']\n",
    "\n",
    "X_train = []\n",
    "for stc in news_train:\n",
    "    token = []\n",
    "    words = stc.split()\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            token.append(word)\n",
    "    X_train.append(token)\n",
    "\n",
    "X_test = []\n",
    "for stc in news_test:\n",
    "    token = []\n",
    "    words = stc.split()\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            token.append(word)\n",
    "    X_test.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2TdBHcwHA7D"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 헤드라인 정수인코딩\n",
    "tokenizer = Tokenizer(25000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBKVMIAPHCXQ"
   },
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "\n",
    "wc = 0\n",
    "for word, word_count in tokenizer.word_counts.items():\n",
    "    if word_count <= 2:\n",
    "        wc += 1\n",
    "\n",
    "print(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdmyLiGHHEUx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len_stc = []\n",
    "for data in X_train:\n",
    "    len_stc.append(len(data))\n",
    "\n",
    "y, x, _ = plt.hist(len_stc, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FlbIpERHFuS"
   },
   "outputs": [],
   "source": [
    "max_len = 15\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgB8_3_DHG6b"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(25000, 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(41, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "loIwsdJhHIKz"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdsCQWOQHJb2"
   },
   "outputs": [],
   "source": [
    "sentence = input()\n",
    "token_stc = sentence.split()\n",
    "encode_stc = tokenizer.texts_to_sequences([token_stc])\n",
    "pad_stc = pad_sequences(encode_stc, maxlen=15)\n",
    "\n",
    "score = model.predict(pad_stc)\n",
    "print(category_list[score.argmax()], score[0, score.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfu2fx0uBYAN"
   },
   "source": [
    "# 트위터 감정 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edlWvF6pBXSo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweet_data = pd.read_csv('./drive/My Drive/tweet_dataset.csv')\n",
    "# print(tweet_data)\n",
    "tweet_data = tweet_data.loc[: , [\"sentiment\", \"text\"]]\n",
    "# print(tweet_data)\n",
    "tweet_data = tweet_data.dropna(how='any')\n",
    "\n",
    "# 감정 인코딩\n",
    "print(pd.factorize(tweet_data['sentiment']))\n",
    "tweet_data['sentiment'] = pd.factorize(tweet_data['sentiment'])[0]\n",
    "print(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z6u9d0gDzBC"
   },
   "outputs": [],
   "source": [
    "# 정규표현식 써서 단어 아니면 삭제\n",
    "tweet_data['text'] = tweet_data['text'].str.replace(\"[^\\w]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzHtJM_REGWv"
   },
   "outputs": [],
   "source": [
    "# test/train 스플릿\n",
    "tweet_train, tweet_test, y_train, y_test = train_test_split(tweet_data['text'], tweet_data['sentiment'], test_size=0.2, shuffle=True, random_state=23)\n",
    "\n",
    "# 원핫벡터화\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(len(y_train[0]))\n",
    "print(len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aT4fkggfEXNB"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# nltk stopwords 리스트\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "# 토큰화 진행\n",
    "X_train = []\n",
    "for stc in tweet_train:\n",
    "    token = []\n",
    "    words = WordPunctTokenizer().tokenize(stc)\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords:\n",
    "            token.append(word.lower())\n",
    "    X_train.append(token)\n",
    "\n",
    "X_test = []\n",
    "for stc in tweet_test:\n",
    "    token = []\n",
    "    words = WordPunctTokenizer().tokenize(stc)\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords:\n",
    "            token.append(word.lower())\n",
    "    X_test.append(token)\n",
    "\n",
    "# print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nhk_ybdG9QX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 트윗 정수인코딩\n",
    "tokenizer = Tokenizer(10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHgUuJFCRTAx"
   },
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUwhgWgKLFbs"
   },
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "low_count = 0\n",
    "for word, word_count in tokenizer.word_counts.items():\n",
    "    if word_count > 1:\n",
    "        low_count += 1\n",
    "print(low_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pabN-n4G-Y1"
   },
   "outputs": [],
   "source": [
    "# 문장의 최대 길이 구하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len_stc = []\n",
    "for data in X_train:\n",
    "    len_stc.append(len(data))\n",
    "\n",
    "y, x, _ = plt.hist(len_stc)\n",
    "plt.show()\n",
    "print(sum(len_stc)/len(len_stc))\n",
    "print(y.max())\n",
    "print(x.max())\n",
    "print(x[np.where(y == y.max())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnwtDL_0HACJ"
   },
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smdx8t8KIaMw"
   },
   "outputs": [],
   "source": [
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "082QQAIUHA5M"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(13, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOjJ75nWK_m3"
   },
   "outputs": [],
   "source": [
    "# 함수형 케라스\n",
    "inputs = Input(shape=(25,))\n",
    "embed = Embedding(10000, 32)(inputs)\n",
    "drop = Dropout(0.3)(embed)\n",
    "\n",
    "# 모델 합성\n",
    "concat_layers = []\n",
    "\n",
    "conv = Conv1D(32, 1, padding='same', activation='relu')(drop)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "flat = Flatten()(pool)\n",
    "concat_layers.append(flat)\n",
    "\n",
    "conv = Conv1D(32, 2, padding='same', activation='relu')(drop)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "flat = Flatten()(pool)\n",
    "concat_layers.append(flat)\n",
    "\n",
    "conv = Conv1D(32, 3, padding='same', activation='relu')(drop)\n",
    "pool = GlobalMaxPooling1D()(conv)\n",
    "flat = Flatten()(pool)\n",
    "concat_layers.append(flat)\n",
    "######\n",
    "concat = Concatenate()(concat_layers)\n",
    "relu = Dense(64, activation='relu')(concat)\n",
    "drop = Dropout(0.5)(relu)\n",
    "\n",
    "outputs = Dense(13, activation=\"softmax\")(drop)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzlQbJZxHCVp"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZXatJOiHDxl"
   },
   "outputs": [],
   "source": [
    "sentence = input()\n",
    "# 토큰화\n",
    "token_stc = sentence.split()\n",
    "# 정수 인코딩\n",
    "encode_stc = tokenizer.texts_to_sequences([token_stc])\n",
    "# 패딩\n",
    "pad_stc = pad_sequences(encode_stc, maxlen = 25)\n",
    "\n",
    "score = model.predict(pad_stc)\n",
    "\n",
    "sent_list = ['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise', 'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger']\n",
    "\n",
    "print(sent_list[score.argmax()], score[0, score.argmax()])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled4.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
