{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOEz8JjR-gQj"
   },
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDSFztmqkoZM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WnVchVxaSF9"
   },
   "outputs": [],
   "source": [
    "files = open(\"./drive/My Drive/par_corp.csv\")\n",
    "\n",
    "re_lines = []\n",
    "for line in files:\n",
    "    if line[0] == '[':\n",
    "        continue\n",
    "    re_line = re.sub('[#\".?!\\n]', '', line)\n",
    "    re_lines.append(re_line)\n",
    "\n",
    "kor = []\n",
    "eng = []\n",
    "count = 0\n",
    "for line in re_lines:\n",
    "    count = count + 1\n",
    "    if count % 2 == 0:\n",
    "        kor.append(line)\n",
    "    else:\n",
    "        eng.append(line)\n",
    "\n",
    "d = {'kor':kor, 'eng':eng}\n",
    "par_corp = pd.DataFrame(d)\n",
    "\n",
    "print(par_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqcW5goHuRSZ"
   },
   "outputs": [],
   "source": [
    "encoder_input, decoder_input, decoder_output = [], [], []\n",
    "\n",
    "# '나는 개와 산책을 하고 있다'\n",
    "# ######## 위 문장의 셀 상태랑 은닉 상태 + <start> 가 인풋으로 들어가면\n",
    "# '<start> i am taking a walk with my dog' -> 각 시점마다 이 문장의 일부분을 decoder_output을 추측하는데 사용하고 있음\n",
    "# 'i am taking a walk with my dog <end>'\n",
    "\n",
    "for stc in par_corp['kor']:\n",
    "    encoder_input.append(stc.split())\n",
    "\n",
    "# 스타트 뒤에 띄어쓰기 있습니다\n",
    "for stc in par_corp['eng']:\n",
    "    decoder_input.append((\"<start> \"+stc).split())\n",
    "\n",
    "# 엔드 앞에 띄어쓰기 있습니다\n",
    "for stc in par_corp['eng']:\n",
    "    decoder_output.append((stc+\" <end>\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRtGMp3KvvT1"
   },
   "outputs": [],
   "source": [
    "tokenizer_ko = Tokenizer()\n",
    "tokenizer_ko.fit_on_texts(encoder_input)\n",
    "encoder_input = tokenizer_ko.texts_to_sequences(encoder_input)\n",
    "\n",
    "# 만약에 5000이면, 1~4999(패딩하기 전) -> 0~4999(패딩하고 난 뒤)\n",
    "'''\n",
    "fit_on_texts를 decoder input output에 대해 둘 다 해주는데 그럼 같은 tokenizer 안에 처음에\n",
    "input에서 fit햇던 거에서 교집합은 그대로 두고  그 같은 output에만 있는 단어(end가 유일하다고 생각합니다)만 추가되는 건가요? -> 네\n",
    "'''\n",
    "tokenizer_en = Tokenizer()\n",
    "tokenizer_en.fit_on_texts(decoder_input)\n",
    "tokenizer_en.fit_on_texts(decoder_output)\n",
    "decoder_input = tokenizer_en.texts_to_sequences(decoder_input)\n",
    "decoder_output = tokenizer_en.texts_to_sequences(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1hcwCmuwhtp"
   },
   "outputs": [],
   "source": [
    "# 문장 길이 체크\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len_ko = []\n",
    "for data in encoder_input:\n",
    "    len_ko.append(len(data))\n",
    "\n",
    "len_en = []\n",
    "for data in decoder_input:\n",
    "    len_en.append(len(data))\n",
    "\n",
    "plt.hist(len_ko, label='ko', alpha=0.7)\n",
    "plt.hist(len_en, label='en', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5PXZxh2xiCg"
   },
   "outputs": [],
   "source": [
    "# maxlen 없어도 알아서 잘 패딩합니다\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_output = pad_sequences(decoder_output, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hll0xCIbaQSv"
   },
   "outputs": [],
   "source": [
    "print(encoder_input.shape)\n",
    "print(decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0Fjxoc_yEhG"
   },
   "outputs": [],
   "source": [
    "# 나중에 prediction 할때 사용하기 위함 (인덱스로 단어 찾기)\n",
    "en_to_index = tokenizer_en.word_index\n",
    "index_to_en = tokenizer_en.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tHcgptHyW4B"
   },
   "outputs": [],
   "source": [
    "test_size = 12000\n",
    "encoder_input_train = encoder_input[:-test_size]\n",
    "decoder_input_train = decoder_input[:-test_size]\n",
    "decoder_output_train = decoder_output[:-test_size]\n",
    "\n",
    "encoder_input_test = encoder_input[-test_size:]\n",
    "decoder_input_test = decoder_input[-test_size:]\n",
    "decoder_output_test = decoder_output[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15K1bAFh9qFD"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2FIv4T0zFJH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ju_5mWXqzIiY"
   },
   "outputs": [],
   "source": [
    "# 인코더 - 한글 문장 받아서 LSTM 마지막 시점의 은닉상태/셀상태 리턴하도록\n",
    "# 원래는 데이터 갯수랑 문장 길이 같이 들어가야함\n",
    "# 왜 데이터 갯수는 명시하지 않을까요?\n",
    "# fit 할때 validation data -> test set -> 데이터 갯수 다르기 때문에\n",
    "encoder_inputs = Input(shape=(27,)) #27은 문장의 길이\n",
    "# +1을 해서 패딩까지 고려\n",
    "encoder_embed = Embedding(len(tokenizer_ko.word_index)+1, 50)(encoder_inputs)\n",
    "# 패딩 값은 필요없는데... (0에 해당하는 임베딩 벡터 제외)\n",
    "encoder_mask = Masking(mask_value=0)(encoder_embed)\n",
    "# return state를 쓰면 마지막 은닉 상태, 마지막 은닉 상태, 마지막 셀 상태 값을 리턴\n",
    "encoder_outputs, h_state, c_state = LSTM(50, return_state=True)(encoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXaPcHYY1BCa"
   },
   "outputs": [],
   "source": [
    "# 디코더 - 위에서 리턴한 상태값이랑, 영어 문장 입력받아서 LSTM의 출력값 리턴하도록\n",
    "decoder_inputs = Input(shape=(27,))\n",
    "decoder_embed = Embedding(len(tokenizer_en.word_index)+1, 50)(decoder_inputs)\n",
    "decoder_mask = Masking(mask_value=0)(decoder_embed)\n",
    "# return sequences를 쓰면 전체 시점의 은닉 상태 값을 리턴\n",
    "# 둘 다 쓰면 전체 시점의 은닉 상태(단어갯수만큼)/마지막 은닉 상태/마지막 셀 상태 값을 리턴\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_mask, initial_state=[h_state, c_state])\n",
    "decoder_dense = Dense(len(tokenizer_en.word_index)+1, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBJDA4pQ1W7C"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "# sparse는 라벨이 정수 형태로 제공될 때 사용되는 함수 (그냥 categorical은 원핫 벡터로 라벨이 제공될 때)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# 레이어 별로 가중치가 학습되는 것임\n",
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_output_train, validation_data = ([encoder_input_test, decoder_input_test], decoder_output_test), batch_size = 128, epochs = 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
