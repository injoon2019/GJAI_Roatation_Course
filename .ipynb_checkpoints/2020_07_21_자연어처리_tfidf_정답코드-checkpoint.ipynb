{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '밥을', '먹었다', '어제의', '밥은', '정말', '맛이', '없었다', '어떻게', '하면', '맛있는', '밥을', '먹을', '수', '있을까']\n",
      "['나', '는', '밥', '을', '먹었다', '어제', '의', '밥', '은', '정말', '맛', '이', '없었다', '어떻게', '하면', '맛있는', '밥', '을', '먹을', '수', '있을까']\n",
      "   하면  먹을  밥을  없었다  맛있는  수  정말  나는  어떻게  먹었다  밥은  어제의  맛이  있을까\n",
      "0   0   0   1    0    0  0   0   1    0    1   0    0   0    0\n",
      "1   0   0   0    1    0  0   1   0    0    0   1    1   1    0\n",
      "2   1   1   1    0    1  1   0   0    1    0   0    0   0    1\n",
      "          idf\n",
      "하면   1.405465\n",
      "먹을   1.405465\n",
      "밥을   1.000000\n",
      "없었다  1.405465\n",
      "맛있는  1.405465\n",
      "수    1.405465\n",
      "정말   1.405465\n",
      "나는   1.405465\n",
      "어떻게  1.405465\n",
      "먹었다  1.405465\n",
      "밥은   1.405465\n",
      "어제의  1.405465\n",
      "맛이   1.405465\n",
      "있을까  1.405465\n",
      "         하면        먹을   밥을       없었다       맛있는         수        정말        나는  \\\n",
      "0  0.000000  0.000000  1.0  0.000000  0.000000  0.000000  0.000000  1.405465   \n",
      "1  0.000000  0.000000  0.0  1.405465  0.000000  0.000000  1.405465  0.000000   \n",
      "2  1.405465  1.405465  1.0  0.000000  1.405465  1.405465  0.000000  0.000000   \n",
      "\n",
      "        어떻게       먹었다        밥은       어제의        맛이       있을까  \n",
      "0  0.000000  1.405465  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.000000  1.405465  1.405465  1.405465  0.000000  \n",
      "2  1.405465  0.000000  0.000000  0.000000  0.000000  1.405465  \n"
     ]
    }
   ],
   "source": [
    "# !pip install konlpy\n",
    "\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# doc_list = [\n",
    "#     '안녕하세요 감사해요 잘있어요 다시 만나요',\n",
    "#     '안녕하세요 오늘도 좋은 하루 보내세요',\n",
    "#     '안녕하세요 안녕하세요 안녕하세요 오늘도 날씨가 좋네요',\n",
    "# ]\n",
    "\n",
    "doc_list = ['나는 밥을 먹었다', '어제의 밥은 정말 맛이 없었다', '어떻게 하면 맛있는 밥을 먹을 수 있을까']\n",
    "# 문서 전체의 단어들을 토큰화\n",
    "#token_list = Okt().morphs(' '.join(doc_list))\n",
    "#token_list2 = Okt().morphs(' '.join(doc_list))\n",
    "token_list = ['나는', '밥을', '먹었다', '어제의','밥은', '정말', '맛이', '없었다', '어떻게', '하면', '맛있는', '밥을', '먹을', '수', '있을까']\n",
    "print(token_list)\n",
    "#print(token_list2)\n",
    "# 중복 단어를 제거하고자\n",
    "token_list = list(set(token_list))\n",
    "\n",
    "def tf(term, document):\n",
    "    return document.count(term)\n",
    "\n",
    "def idf(term):\n",
    "    # 함수가 돌 때마다 초기화\n",
    "    df = 0\n",
    "    for doc in doc_list:\n",
    "        # 각 문서마다 해당 단어가 있는지 확인\n",
    "        if term in doc:\n",
    "            df = df + 1\n",
    "    return log(len(doc_list)/(df+1))+1\n",
    "\n",
    "def tfidf(term, document):\n",
    "    return tf(term, document)*idf(term)\n",
    "\n",
    "dtm = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    # 방금 추가한 문서의 리스트\n",
    "    # 문서로서의 행\n",
    "    dtm.append([])\n",
    "    for token in token_list:\n",
    "        # -1은 마지막 인덱스\n",
    "        dtm[-1].append(tf(token, doc))\n",
    "\n",
    "# [[]]\n",
    "# [[단어1의 tf, 단어2의 tf ....]] -> 문서1 역할을 할 행\n",
    "# [[단어1의 tf ~~~~...], [단어1의 tf...], []]\n",
    "\n",
    "# print(token_list)\n",
    "# print(dtm)\n",
    "# print('\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "dtm_pd = pd.DataFrame(dtm, columns=token_list)\n",
    "print(dtm_pd)\n",
    "\n",
    "idf_list = []\n",
    "\n",
    "for token in token_list:\n",
    "    idf_list.append(idf(token))\n",
    "\n",
    "idf_pd = pd.DataFrame(idf_list, columns=['idf'], index=token_list)\n",
    "print(idf_pd)\n",
    "\n",
    "tfidf_list = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    tfidf_list.append([])\n",
    "    for token in token_list:\n",
    "        tfidf_list[-1].append(tfidf(token, doc))\n",
    "\n",
    "tfidf_pd = pd.DataFrame(tfidf_list, columns=token_list)\n",
    "print(tfidf_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
